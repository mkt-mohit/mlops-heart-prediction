â¤ï¸ Heart Disease Prediction â€” End-to-End MLOps Project
ğŸ“Œ Introduction

This project demonstrates a complete end-to-end MLOps pipeline for building, validating, deploying, and monitoring a machine learning model that predicts heart disease risk using the UCI Heart Disease dataset.

The project follows modern MLOps best practices, including:

Reproducible data preprocessing and model training

Experiment tracking

Automated CI/CD

Containerized model serving

Cloud-native production deployment

Centralized logging and monitoring

The final model is exposed as a REST API deployed on Google Cloud Run.

ğŸ¯ Problem Statement

Given patient health attributes (age, cholesterol, blood pressure, ECG results, etc.), predict whether the patient has a risk of heart disease.

Target variable

0 â†’ No heart disease

1 â†’ Heart disease present

ğŸ—ï¸ Architecture Overview
GitHub Repository
        â”‚
        â”‚  (git push)
        â–¼
GitHub Actions (CI/CD)
 â”œâ”€ Linting (flake8)
 â”œâ”€ Unit tests (pytest)
 â”œâ”€ Model training & validation
 â””â”€ Cloud Run deployment
        â”‚
        â–¼
Google Cloud Run
 â”œâ”€ FastAPI REST API
 â”œâ”€ Docker container
 â”œâ”€ Auto-scaling
 â””â”€ Cloud Logging & Metrics

ğŸ§° Tech Stack
Category	Tools
Language	Python 3.12
Machine Learning	scikit-learn
Experiment Tracking	MLflow
API Framework	FastAPI
Containerization	Docker
CI/CD	GitHub Actions
Cloud Platform	Google Cloud (Cloud Run)
Monitoring & Logging	Cloud Logging, Cloud Run Metrics
ğŸ“‚ Project Structure
mlops_heart_disease/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py               # FastAPI application
â”‚   â””â”€â”€ schemas.py            # Request/response schemas
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ artifacts/
â”‚       â””â”€â”€ random_forest_final.pkl
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data.py
â”‚   â””â”€â”€ test_model.py
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yaml               # CI/CD pipeline
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md

ğŸ”„ MLOps Workflow
Step-1: Data Acquisition & EDA

Dataset downloaded from UCI repository

Missing values handled

Target converted to binary

EDA performed

Clean dataset saved for reuse

Step-2: Feature Engineering & Model Training

Feature scaling and encoding

Logistic Regression and Random Forest trained

Cross-validation used for model selection

Step-3: Experiment Tracking

MLflow used to log parameters, metrics, and models

Enables reproducibility and comparison

Step-4: Model Packaging

Final model saved as a pipeline (preprocessing + model)

Prevents training-serving skew

Step-5: CI/CD Automation

GitHub Actions pipeline includes:

Linting

Unit tests

Model training (sanity check)

Step-6: Model Containerization

FastAPI-based inference service

Dockerized for portability

/predict endpoint returns prediction and confidence

Step-7: Production Deployment

Deployed to Google Cloud Run

Fully serverless and auto-scaling

Deployment triggered automatically via CI/CD

Step-8: Monitoring & Logging

Application logs emitted via Python logging

Logs captured in Cloud Logging

Metrics visible via Cloud Run dashboard

ğŸš€ Deployment Instructions
Prerequisites

Google Cloud account

GitHub account

Billing enabled on GCP

GitHub repository cloned

1ï¸âƒ£ Clone the Repository
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>

2ï¸âƒ£ CI/CD & Cloud Deployment (Recommended)

No local dependency installation is required

All dependencies listed in requirements.txt are automatically installed during the CI/CD pipeline and container build.

Simply push code to the main branch:

git push origin main


GitHub Actions will:

Run tests and linting

Build the container

Deploy the API to Google Cloud Run

3ï¸âƒ£ Local Development (Optional)

If you want to run the API locally for testing or debugging:

python -m venv venv
source venv/bin/activate      # Linux / macOS
# venv\Scripts\activate       # Windows

pip install -r requirements.txt
uvicorn app.main:app --reload


API will be available at:

http://localhost:8000

ğŸŒ Using the API
Health Check
GET /


Response:

{
  "status": "ok",
  "message": "Heart Disease Model is running"
}

Prediction Endpoint
POST /predict

Sample Request
curl -X POST https://<cloud-run-url>/predict \
-H "Content-Type: application/json" \
-d '{
  "age": 67,
  "sex": 1,
  "cp": 3,
  "trestbps": 180,
  "chol": 320,
  "fbs": 1,
  "restecg": 2,
  "thalach": 90,
  "exang": 1,
  "oldpeak": 3.5,
  "slope": 0,
  "ca": 3,
  "thal": 3
}'

Sample Response
{
  "prediction": 1,
  "confidence": 0.84
}

ğŸ§ª Testing

Run unit tests locally:

pytest


Run linting:

flake8

ğŸ“Š Monitoring & Logs
Logs

Google Cloud Console â†’ Cloud Run â†’ Service â†’ Logs

Or Logs Explorer with:

resource.type="cloud_run_revision"

Metrics

Cloud Run â†’ Service â†’ Metrics

View request rate, latency, CPU, and memory usage

ğŸ” Security & Design Notes

Cloud Run infrastructure logs do not capture request bodies by design

Application logs record sanitized summaries of inputs

Full payload logging is intentionally avoided to prevent PII exposure

CI validates code correctness; deployment is automated separately
